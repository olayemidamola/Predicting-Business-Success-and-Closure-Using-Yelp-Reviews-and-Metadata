{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2DwdY_tFYhf"
      },
      "source": [
        "## 1. Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZICvkZ2XFQMI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbC6Kb_DFndt"
      },
      "source": [
        "## 2. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uviwIT_VWmo2",
        "outputId": "99abadc9-5ac8-4620-a495-1637c7d6a78c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "business: 80000 rows read\n",
            "checkin: 80000 rows read\n",
            "photos: 80000 rows read\n",
            "review: 80000 rows read\n",
            "tip: 80000 rows read\n",
            "user: 80000 rows read\n"
          ]
        }
      ],
      "source": [
        "files = {\n",
        "    \"business\": \"/content/drive/MyDrive/DAB322_Capstone 1_Group 9/Yelp_Datasets/Original_JSON_files/yelp_academic_dataset_business.json\",\n",
        "    \"checkin\": \"/content/drive/MyDrive/DAB322_Capstone 1_Group 9/Yelp_Datasets/Original_JSON_files/yelp_academic_dataset_checkin.json\",\n",
        "    \"photos\": \"/content/drive/MyDrive/DAB322_Capstone 1_Group 9/Yelp_Datasets/Original_JSON_files/photos.json\",\n",
        "    \"review\": \"/content/drive/MyDrive/DAB322_Capstone 1_Group 9/Yelp_Datasets/Original_JSON_files/yelp_academic_dataset_review.json\",\n",
        "    \"tip\": \"/content/drive/MyDrive/DAB322_Capstone 1_Group 9/Yelp_Datasets/Original_JSON_files/yelp_academic_dataset_tip.json\",\n",
        "    \"user\": \"/content/drive/MyDrive/DAB322_Capstone 1_Group 9/Yelp_Datasets/Original_JSON_files/yelp_academic_dataset_user.json\"\n",
        "}\n",
        "\n",
        "# limit the number of rows\n",
        "limit = 80000\n",
        "\n",
        "datasets = {}\n",
        "for name, path in files.items():\n",
        "    df = pd.read_json(path, lines=True, nrows=limit)\n",
        "    datasets[name] = df\n",
        "    print(f\"{name}: {len(df)} rows read\")\n",
        "\n",
        "# files name\n",
        "business = datasets[\"business\"]\n",
        "checkin = datasets[\"checkin\"]\n",
        "photos = datasets[\"photos\"]\n",
        "review = datasets[\"review\"]\n",
        "tip = datasets[\"tip\"]\n",
        "user = datasets[\"user\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63EDiGKgGHIy"
      },
      "source": [
        "## 4. Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du9Xbo-OTU3h"
      },
      "source": [
        "### 4.1. Business"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJFKl7LCZ3h9",
        "outputId": "8990fee2-b1b9-45ec-9ac0-77179a7227b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Invalid business star ratings:\n",
            "Empty DataFrame\n",
            "Columns: [business_id, name, address, city, state, postal_code, latitude, longitude, stars, review_count, is_open, attributes, categories, hours]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "# Handle missing values in optional business fields\n",
        "business['categories'] = business['categories'].fillna('Unknown')\n",
        "business['address'] = business['address'].fillna('Unknown')\n",
        "business['postal_code'] = business['postal_code'].fillna('Unknown')\n",
        "business['attributes'] = business['attributes'].fillna('Unknown')\n",
        "business['hours'] = business['hours'].fillna('Unknown')\n",
        "\n",
        "# Standardize text formatting\n",
        "business['city'] = business['city'].str.title()\n",
        "business['state'] = business['state'].str.upper()\n",
        "\n",
        "# Validate star ratings (must be between 0 and 5)\n",
        "invalid_stars = business[(business['stars'] > 5) | (business['stars'] < 0)]\n",
        "print(f\"Invalid business star ratings:\\n{invalid_stars}\")\n",
        "\n",
        "# Extract primary category from category list\n",
        "business['main_cat'] = business['categories'].apply(\n",
        "    lambda x: str(x).split(',')[0].strip() if pd.notnull(x) else 'Unknown'\n",
        ")\n",
        "\n",
        "# Normalize categories into high-level groups\n",
        "def map_business_category(category):\n",
        "    category = str(category).lower()\n",
        "\n",
        "    if any(k in category for k in ['restaurant', 'food', 'cafe', 'pizza', 'coffee']):\n",
        "        return 'Food & Beverage'\n",
        "    if any(k in category for k in ['health', 'clinic', 'fitness', 'spa']):\n",
        "        return 'Health & Wellness'\n",
        "    if any(k in category for k in ['hotel', 'inn', 'bnb']):\n",
        "        return 'Accommodation'\n",
        "    if any(k in category for k in ['bar', 'nightlife', 'pub']):\n",
        "        return 'Nightlife'\n",
        "    if any(k in category for k in ['shop', 'store', 'retail']):\n",
        "        return 'Retail'\n",
        "    if any(k in category for k in ['auto', 'car', 'mechanic']):\n",
        "        return 'Automotive'\n",
        "    return 'Other'\n",
        "\n",
        "business['main_cat'] = business['main_cat'].apply(map_business_category)\n",
        "\n",
        "# Save cleaned business data\n",
        "business.to_csv('business_cleaned.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAohCwkdpHdz"
      },
      "source": [
        "### 4.2. Photos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALLwpIHDZ5Pj",
        "outputId": "3514540c-a621-445a-ac9c-8e20d91e7d8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Photos dataset cleaned and saved.\n"
          ]
        }
      ],
      "source": [
        "# Fill missing optional text fields\n",
        "photos['caption'] = photos['caption'].fillna('No caption')\n",
        "photos['label'] = photos['label'].fillna('Unknown')\n",
        "\n",
        "# Ensure identifier fields are stored as strings\n",
        "photos['photo_id'] = photos['photo_id'].astype(str)\n",
        "photos['business_id'] = photos['business_id'].astype(str)\n",
        "\n",
        "# Remove duplicate photo records\n",
        "photos = photos.drop_duplicates(subset='photo_id')\n",
        "\n",
        "# Save cleaned photos dataset\n",
        "photos.to_csv('photos_cleaned.csv', index=False)\n",
        "\n",
        "print(\"Photos dataset cleaned and saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr8FHG0MrObZ"
      },
      "source": [
        "### 4.3. Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G9OOm0itZ6j_"
      },
      "outputs": [],
      "source": [
        "# Keep only relevant columns\n",
        "review = review[['review_id', 'user_id', 'business_id', 'stars', 'text', 'date']]\n",
        "\n",
        "# Fill missing values (text + stars)\n",
        "review['text'] = review['text'].fillna('No review text')\n",
        "review['stars'] = review['stars'].fillna(0)\n",
        "\n",
        "# Convert date to datetime\n",
        "review['date'] = pd.to_datetime(review['date'], errors='coerce')\n",
        "\n",
        "# Ensure ID columns are strings\n",
        "review['review_id'] = review['review_id'].astype(str)\n",
        "review['user_id'] = review['user_id'].astype(str)\n",
        "review['business_id'] = review['business_id'].astype(str)\n",
        "\n",
        "# Remove duplicate reviews\n",
        "review = review.drop_duplicates(subset='review_id')\n",
        "\n",
        "# Save cleaned review dataset\n",
        "review.to_csv('review_cleaned.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgGzAO9WUR8R"
      },
      "source": [
        "### 4.4. Checkin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xbfNYh47Z72v"
      },
      "outputs": [],
      "source": [
        "# Split comma-separated check-in timestamps into a list\n",
        "checkin['date_list'] = checkin['date'].str.split(',')\n",
        "\n",
        "# Expand list into multiple rows (one row per check-in)\n",
        "checkin = checkin.explode('date_list')\n",
        "\n",
        "# Convert to datetime\n",
        "checkin['checkin_time'] = pd.to_datetime(checkin['date_list'].str.strip(), errors='coerce')\n",
        "\n",
        "# Drop invalid check-in timestamps\n",
        "checkin = checkin.dropna(subset=['checkin_time'])\n",
        "\n",
        "# Extract time features\n",
        "checkin['year'] = checkin['checkin_time'].dt.year\n",
        "checkin['month'] = checkin['checkin_time'].dt.month\n",
        "checkin['day_of_week'] = checkin['checkin_time'].dt.dayofweek\n",
        "checkin['hour'] = checkin['checkin_time'].dt.hour\n",
        "\n",
        "# Drop raw columns that are no longer needed\n",
        "checkin = checkin.drop(columns=['date', 'date_list'])\n",
        "\n",
        "# Save cleaned check-in dataset\n",
        "checkin.to_csv('checkin_cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpWfB4W1qEX3"
      },
      "source": [
        "### 4.5. User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UURTPcj-Z8C_"
      },
      "outputs": [],
      "source": [
        "# Drop high-cardinality column (optional)\n",
        "user_clean = user.drop(columns=['friends'], errors='ignore')\n",
        "\n",
        "# Fill missing name and elite fields\n",
        "user_clean['name'] = user_clean['name'].fillna('Unknown')\n",
        "user_clean['elite'] = user_clean['elite'].fillna('None')\n",
        "\n",
        "# Convert yelping_since to datetime\n",
        "user_clean['yelping_since'] = pd.to_datetime(user_clean['yelping_since'], errors='coerce')\n",
        "\n",
        "# Remove duplicate users\n",
        "user_clean = user_clean.drop_duplicates(subset='user_id')\n",
        "\n",
        "# Save cleaned user dataset\n",
        "user_clean.to_csv('user_cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rVyD6bAsqQQ"
      },
      "source": [
        "### 4.6. Tip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cfiwjy_1Z-Hy"
      },
      "outputs": [],
      "source": [
        "# Remove empty tip text\n",
        "tip = tip[tip['text'].str.strip().notna()]\n",
        "\n",
        "# Convert date to datetime\n",
        "tip['date'] = pd.to_datetime(tip['date'], errors='coerce')\n",
        "\n",
        "# Drop rows missing key identifiers\n",
        "tip = tip.dropna(subset=['user_id', 'business_id'])\n",
        "\n",
        "# Remove duplicate tips\n",
        "tip = tip.drop_duplicates(subset=['user_id', 'business_id', 'text'])\n",
        "\n",
        "# Save cleaned tip dataset\n",
        "tip.to_csv('tip_cleaned.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kFsKZYsYF1r3",
        "63EDiGKgGHIy",
        "du9Xbo-OTU3h"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
